{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "fake_news_classification.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "kXOLeWD0DcJl",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "outputId": "531a6bca-b9c7-4fed-d1a9-03882c170681"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import pandas as pd\n",
    "df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/merged.csv')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lMhrk5fcDy1B",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "outputId": "c873a55c-ac29-4b21-c5a6-438bbf37bddc"
   },
   "source": [
    "df.head(10)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Racist Alabama Cops Brutalize Black Boy While...</td>\n",
       "      <td>The number of cases of cops brutalizing and ki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fresh Off The Golf Course, Trump Lashes Out A...</td>\n",
       "      <td>Donald Trump spent a good portion of his day a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Trump Said Some INSANELY Racist Stuff Inside ...</td>\n",
       "      <td>In the wake of yet another court decision that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Former CIA Director Slams Trump Over UN Bully...</td>\n",
       "      <td>Many people have raised the alarm regarding th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WATCH: Brand-New Pro-Trump Ad Features So Muc...</td>\n",
       "      <td>Just when you might have thought we d get a br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  ... label\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...  ...     1\n",
       "1   Drunk Bragging Trump Staffer Started Russian ...  ...     1\n",
       "2   Sheriff David Clarke Becomes An Internet Joke...  ...     1\n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...     1\n",
       "4   Pope Francis Just Called Out Donald Trump Dur...  ...     1\n",
       "5   Racist Alabama Cops Brutalize Black Boy While...  ...     1\n",
       "6   Fresh Off The Golf Course, Trump Lashes Out A...  ...     1\n",
       "7   Trump Said Some INSANELY Racist Stuff Inside ...  ...     1\n",
       "8   Former CIA Director Slams Trump Over UN Bully...  ...     1\n",
       "9   WATCH: Brand-New Pro-Trump Ad Features So Muc...  ...     1\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k-_vxafVZqNr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QhzMLQw4EEu7",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "fe89e932-245b-41f5-ba07-5cbe7c482cdd"
   },
   "source": [
    "from getEmbeddings import getEmbeddings\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "#import scikitplot.plotters as skplt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_cmat(yte, ypred):\n",
    "    '''Plotting confusion matrix'''\n",
    "    skplt.plot_confusion_matrix(yte,ypred)\n",
    "    plt.show()\n",
    "\n",
    "'''# Read the data\n",
    "if not os.path.isfile('./xtr.npy') or \\\n",
    "    not os.path.isfile('./xte.npy') or \\\n",
    "    not os.path.isfile('./ytr.npy') or \\\n",
    "    not os.path.isfile('./yte.npy'):\n",
    "    xtr,xte,ytr,yte = getEmbeddings('/content/drive/My Drive/Colab Notebooks/merged.csv')\n",
    "    np.save('./xtr', xtr)\n",
    "    np.save('./xte', xte)\n",
    "    np.save('./ytr', ytr)\n",
    "    np.save('./yte', yte)\n",
    "'''\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/yte.npy')\n",
    "\n",
    "# SVM classifier\n",
    "clf = SVC()\n",
    "clf.fit(xtr, ytr)\n",
    "y_pred = clf.predict(xte)\n",
    "m = yte.shape[0]\n",
    "n = (yte != y_pred).sum()\n",
    "print(\"Accuracy = \" + format((m-n)/m*100, '.2f') + \"%\")  \n",
    "\n",
    "# Draw the confusion matrix\n",
    "#plot_cmat(yte, y_pred)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Accuracy = 91.36%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V9KZUH7jyTa1",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "bf31a480-8c27-46a8-fbc5-40bc4c248220"
   },
   "source": [
    "\"\"\"\n",
    "Fake news detection\n",
    "The naive bayes model\n",
    "\"\"\"\n",
    "\n",
    "from getEmbeddings import getEmbeddings\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import scikitplot.plotters as skplt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_cmat(yte, ypred):\n",
    "    '''Plotting confusion matrix'''\n",
    "    skplt.plot_confusion_matrix(yte,ypred)\n",
    "    plt.show()\n",
    "\n",
    "# Read the data\n",
    "'''if not os.path.isfile('./xtr.npy') or \\\n",
    "    not os.path.isfile('./xte.npy') or \\\n",
    "    not os.path.isfile('./ytr.npy') or \\\n",
    "    not os.path.isfile('./yte.npy'):\n",
    "    xtr,xte,ytr,yte = getEmbeddings(\"datasets/train.csv\")\n",
    "    np.save('./xtr', xtr)\n",
    "    np.save('./xte', xte)\n",
    "    np.save('./ytr', ytr)\n",
    "    np.save('./yte', yte)\n",
    "'''\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/yte.npy')\n",
    "\n",
    "# Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(xtr,ytr)\n",
    "y_pred = gnb.predict(xte)\n",
    "m = yte.shape[0]\n",
    "n = (yte != y_pred).sum()\n",
    "print(\"Accuracy = \" + format((m-n)/m*100, '.2f') + \"%\")\n",
    "\n",
    "# Draw the confusion matrix\n",
    "#plot_cmat(yte, y_pred)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Accuracy = 72.12%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ENExMzA9LY9y",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "b43a549c-e933-4062-9d2a-8e15187887dc"
   },
   "source": [
    "\"\"\"\n",
    "Fake news detection\n",
    "The decison tree model\n",
    "\"\"\"\n",
    "\n",
    "from getEmbeddings import getEmbeddings\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import scikitplot.plotters as skplt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_cmat(yte, ypred):\n",
    "    '''Plotting confusion matrix'''\n",
    "    skplt.plot_confusion_matrix(yte,ypred)\n",
    "    plt.show()\n",
    "\n",
    "# Read the data\n",
    "'''if not os.path.isfile('./xtr.npy') or \\\n",
    "    not os.path.isfile('./xte.npy') or \\\n",
    "    not os.path.isfile('./ytr.npy') or \\\n",
    "    not os.path.isfile('./yte.npy'):\n",
    "    xtr,xte,ytr,yte = getEmbeddings(\"datasets/train.csv\")\n",
    "    np.save('./xtr', xtr)\n",
    "    np.save('./xte', xte)\n",
    "    np.save('./ytr', ytr)\n",
    "    np.save('./yte', yte)\n",
    "'''\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/yte.npy')\n",
    "\n",
    "#Decision Tree classifier\n",
    "model = tree.DecisionTreeClassifier(max_depth=10)\n",
    "model.fit(xtr, ytr)\n",
    "y_pred = model.predict(xte)\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#accuracy_score(yte, y_pred)\n",
    "m = yte.shape[0]\n",
    "n = (yte != y_pred).sum()\n",
    "print(\"Accuracy = \" + format((m-n)/m*100, '.2f') + \"%\")\n",
    "\n",
    "## {max_depth=10:accuracy:80.98%,\n",
    "#   max_depth=2:accuracy:~75%,\n",
    "#   max_depth=15:accuracy:79.85%, }"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Accuracy = 81.00%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from getEmbeddings import getEmbeddings\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "\n",
    "'''# Read the data\n",
    "if not os.path.isfile('./xtr.npy') or \\\n",
    "    not os.path.isfile('./xte.npy') or \\\n",
    "    not os.path.isfile('./ytr.npy') or \\\n",
    "    not os.path.isfile('./yte.npy'):\n",
    "    xtr,xte,ytr,yte = getEmbeddings('/content/drive/My Drive/Colab Notebooks/merged.csv')\n",
    "    np.save('./xtr', xtr)\n",
    "    np.save('./xte', xte)\n",
    "    np.save('./ytr', ytr)\n",
    "    np.save('./yte', yte)\n",
    "'''\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/yte.npy')\n",
    "\n",
    "# RandomForest for classification\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(xtr, ytr)\n",
    "y_pred = clf.predict(xte)\n",
    "m = yte.shape[0]\n",
    "n = (yte != y_pred).sum()\n",
    "print(\"Accuracy = \" + format((m-n)/m*100, '.2f') + \"%\")  \n",
    "\n",
    "# Draw the confusion matrix\n",
    "#plot_cmat(yte, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Fake news detection\n",
    "Logistic regression model\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/fake_newsdetection/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/fake_newsdetection/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/fake_newsdetection/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/fake_newsdetection/yte.npy')\n",
    "\n",
    "logmodel = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logmodel.fit(xtr, ytr)\n",
    "predictions = logmodel.predict(xte)\n",
    "confusion_matrix_res = confusion_matrix(yte, predictions)\n",
    "# print(confusion_matrix_res)\n",
    "diagonal_sum = confusion_matrix_res.trace()\n",
    "sum_of_all_elements = confusion_matrix_res.sum()\n",
    "print(\"Accuracy = \" + format((diagonal_sum / sum_of_all_elements )*100, '.2f') + \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fake news detection\n",
    "The Keras version of neural network\n",
    "\"\"\"\n",
    "\n",
    "from getEmbeddings import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Read the data\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/yte.npy')\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    '''Neural network with 3 hidden layers'''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=300, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(80, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(2, activation=\"softmax\", kernel_initializer='normal'))\n",
    "\n",
    "    # gradient descent\n",
    "    epochs=100\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = baseline_model()\n",
    "model.summary()\n",
    "x_train, x_test, y_train, y_test = train_test_split(xtr, ytr, test_size=0.2, random_state=42)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y = np_utils.to_categorical((label_encoder.transform(y_train)))\n",
    "label_encoder.fit(y_test)\n",
    "encoded_y_test = np_utils.to_categorical((label_encoder.transform(y_test)))\n",
    "estimator = model.fit(x_train, encoded_y, epochs=100, batch_size=64)\n",
    "print(\"Model Trained!\")\n",
    "score = model.evaluate(x_test, encoded_y_test)\n",
    "print(\"\")\n",
    "print(\"Accuracy = \" + format(score[1]*100, '.2f') + \"%\")   # 92.69%\n",
    "\n",
    "probabs = model.predict_proba(x_test)\n",
    "y_pred = np.argmax(probabs, axis=1)\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fake news detection\n",
    "The Keras version of neural network\n",
    "\"\"\"\n",
    "\n",
    "from getEmbeddings import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "\n",
    "from keras import losses\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "\n",
    "# Read the data\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/yte.npy')\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    '''Neural network with 3 hidden layers'''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=300, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(80, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(2, activation=\"softmax\", kernel_initializer='normal'))\n",
    "\n",
    "    # gradient descent\n",
    "    epochs=100\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = baseline_model()\n",
    "model.summary()\n",
    "x_train, x_test, y_train, y_test = train_test_split(xtr, ytr, test_size=0.2, random_state=42)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y = np_utils.to_categorical((label_encoder.transform(y_train)))\n",
    "label_encoder.fit(y_test)\n",
    "encoded_y_test = np_utils.to_categorical((label_encoder.transform(y_test)))\n",
    "\n",
    "\n",
    "epochs = 60\n",
    "learning_rate = 0.1 # initial learning rate\n",
    "decay_rate = 0.1\n",
    "momentum = 0.8\n",
    "# define the learning rate change \n",
    "def exp_decay(epoch):\n",
    "    lrate = learning_rate * np.exp(-decay_rate*epoch)\n",
    "    return lrate\n",
    "    \n",
    "# learning schedule callback\n",
    "loss_history = History()\n",
    "lr_rate = LearningRateScheduler(exp_decay)\n",
    "callbacks_list = [loss_history, lr_rate]\n",
    "\n",
    "# you invoke the LearningRateScheduler during the .fit() phase\n",
    "exponential_decay_model_history = model.fit(x_train, encoded_y,\n",
    "                                    batch_size=64,\n",
    "                                    epochs=100,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    verbose=1,\n",
    "                                    validation_data=(x_test, encoded_y_test))\n",
    "\n",
    "#estimator = model.fit(x_train, encoded_y, epochs=100, batch_size=64)\n",
    "print(\"Model Trained!\")\n",
    "score = model.evaluate(x_test, encoded_y_test)\n",
    "print(\"\")\n",
    "print(\"Accuracy = \" + format(score[1]*100, '.2f') + \"%\") \n",
    "probabs = model.predict_proba(x_test)\n",
    "y_pred = np.argmax(probabs, axis=1)\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Fake news detection\n",
    "LSTM model\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import os\n",
    "import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "top_words = 5000\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Read the text data\n",
    "if not os.path.isfile('./xtr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./xte_shuffled.npy') or \\\n",
    "    not os.path.isfile('./ytr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./yte_shuffled.npy'):\n",
    "    getEmbeddings.clean_data()\n",
    "\n",
    "\n",
    "xtr = np.load('./xtr_shuffled.npy')\n",
    "xte = np.load('./xte_shuffled.npy')\n",
    "y_train = np.load('./ytr_shuffled.npy')\n",
    "y_test = np.load('./yte_shuffled.npy')\n",
    "\n",
    "cnt = Counter()\n",
    "x_train = []\n",
    "for x in xtr:\n",
    "    x_train.append(x.split())\n",
    "    for word in x_train[-1]:\n",
    "        cnt[word] += 1  \n",
    "\n",
    "# Storing most common words\n",
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_train:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Delete the short news\n",
    "i = 0\n",
    "while i < len(x_train):\n",
    "    if len(x_train[i]) > 10:\n",
    "        i += 1\n",
    "    else:\n",
    "        del x_train[i]\n",
    "        del y_train[i]\n",
    "\n",
    "# Generating test data\n",
    "x_test = []\n",
    "for x in xte:\n",
    "    x_test.append(x.split())\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_test:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_num, batch_size=batch_size)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy= %.2f%%\" % (scores[1]*100))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KGcMAeAVihM9",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "6d44b17e-ad8a-4eeb-bc81-6bc2a7ea74e5"
   },
   "source": [
    "\n",
    "\"\"\"\n",
    "Fake news detection\n",
    "LSTM model\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import os\n",
    "import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "top_words = 5000\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Read the text data\n",
    "if not os.path.isfile('./xtr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./xte_shuffled.npy') or \\\n",
    "    not os.path.isfile('./ytr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./yte_shuffled.npy'):\n",
    "    getEmbeddings.clean_data()\n",
    "\n",
    "\n",
    "xtr = np.load('./xtr_shuffled.npy')\n",
    "xte = np.load('./xte_shuffled.npy')\n",
    "y_train = np.load('./ytr_shuffled.npy')\n",
    "y_test = np.load('./yte_shuffled.npy')\n",
    "\n",
    "cnt = Counter()\n",
    "x_train = []\n",
    "for x in xtr:\n",
    "    x_train.append(x.split())\n",
    "    for word in x_train[-1]:\n",
    "        cnt[word] += 1  \n",
    "\n",
    "# Storing most common words\n",
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_train:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Delete the short news\n",
    "i = 0\n",
    "while i < len(x_train):\n",
    "    if len(x_train[i]) > 10:\n",
    "        i += 1\n",
    "    else:\n",
    "        del x_train[i]\n",
    "        del y_train[i]\n",
    "\n",
    "# Generating test data\n",
    "x_test = []\n",
    "for x in xte:\n",
    "    x_test.append(x.split())\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_test:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_num, batch_size=batch_size)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy= %.2f%%\" % (scores[1]*100))\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Accuracy = 81.58%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FJ-WksTni2t6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "Fake news detection\n",
    "Logistic regression model\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/fake_newsdetection/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/fake_newsdetection/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/fake_newsdetection/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/fake_newsdetection/yte.npy')\n",
    "\n",
    "logmodel = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logmodel.fit(xtr, ytr)\n",
    "predictions = logmodel.predict(xte)\n",
    "confusion_matrix_res = confusion_matrix(yte, predictions)\n",
    "# print(confusion_matrix_res)\n",
    "diagonal_sum = confusion_matrix_res.trace()\n",
    "sum_of_all_elements = confusion_matrix_res.sum()\n",
    "print(\"Accuracy = \" + format((diagonal_sum / sum_of_all_elements )*100, '.2f') + \"%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pTz_7_bfRKSs",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "de58b942-4056-43e5-f6ea-fd6897c8e028"
   },
   "source": [
    "\"\"\"\n",
    "Fake news detection\n",
    "The Keras version of neural network\n",
    "\"\"\"\n",
    "\n",
    "from getEmbeddings import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Read the data\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/yte.npy')\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    '''Neural network with 3 hidden layers'''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=300, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(80, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(2, activation=\"softmax\", kernel_initializer='normal'))\n",
    "\n",
    "    # gradient descent\n",
    "    epochs=100\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = baseline_model()\n",
    "model.summary()\n",
    "x_train, x_test, y_train, y_test = train_test_split(xtr, ytr, test_size=0.2, random_state=42)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y = np_utils.to_categorical((label_encoder.transform(y_train)))\n",
    "label_encoder.fit(y_test)\n",
    "encoded_y_test = np_utils.to_categorical((label_encoder.transform(y_test)))\n",
    "estimator = model.fit(x_train, encoded_y, epochs=100, batch_size=64)\n",
    "print(\"Model Trained!\")\n",
    "score = model.evaluate(x_test, encoded_y_test)\n",
    "print(\"\")\n",
    "print(\"Accuracy = \" + format(score[1]*100, '.2f') + \"%\")   # 92.69%\n",
    "\n",
    "probabs = model.predict_proba(x_test)\n",
    "y_pred = np.argmax(probabs, axis=1)\n",
    " "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_86 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 80)                20560     \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 2)                 162       \n",
      "=================================================================\n",
      "Total params: 295,154\n",
      "Trainable params: 295,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5537\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.6883 - accuracy: 0.5969\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.6176\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.6328\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.6551\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6754\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.5977 - accuracy: 0.7180\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7803\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.4572 - accuracy: 0.8140\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.4093 - accuracy: 0.8383\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.3782 - accuracy: 0.8504\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.3618 - accuracy: 0.8550\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.3423 - accuracy: 0.8672\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.3361 - accuracy: 0.8640\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.3241 - accuracy: 0.8754\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.3182 - accuracy: 0.8754\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.3070 - accuracy: 0.8796\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.3006 - accuracy: 0.8835\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2953 - accuracy: 0.8847\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2907 - accuracy: 0.8850\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2827 - accuracy: 0.8910\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2827 - accuracy: 0.8900\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2803 - accuracy: 0.8912\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2758 - accuracy: 0.8931\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2667 - accuracy: 0.8947\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2601 - accuracy: 0.8988\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2584 - accuracy: 0.8976\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2594 - accuracy: 0.8986\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2504 - accuracy: 0.9051\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2517 - accuracy: 0.9016\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2422 - accuracy: 0.9067\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2377 - accuracy: 0.9056\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2377 - accuracy: 0.9049\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2316 - accuracy: 0.9073\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2277 - accuracy: 0.9110\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2264 - accuracy: 0.9095\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2256 - accuracy: 0.9128\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9128\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2180 - accuracy: 0.9140\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2141 - accuracy: 0.9183\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2118 - accuracy: 0.9183\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2074 - accuracy: 0.9186\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2052 - accuracy: 0.9198\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2063 - accuracy: 0.9208\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2059 - accuracy: 0.9211\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1999 - accuracy: 0.9222\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2011 - accuracy: 0.9212\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1952 - accuracy: 0.9275\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1911 - accuracy: 0.9268\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1913 - accuracy: 0.9279\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1880 - accuracy: 0.9294\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1833 - accuracy: 0.9287\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1810 - accuracy: 0.9305\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1860 - accuracy: 0.9265\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1823 - accuracy: 0.9317\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1796 - accuracy: 0.9308\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1793 - accuracy: 0.9314\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1775 - accuracy: 0.9315\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1728 - accuracy: 0.9347\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1717 - accuracy: 0.9354\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1688 - accuracy: 0.9363\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1703 - accuracy: 0.9351\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1691 - accuracy: 0.9370\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1688 - accuracy: 0.9375\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1608 - accuracy: 0.9409\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1602 - accuracy: 0.9391\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1598 - accuracy: 0.9404\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1588 - accuracy: 0.9402\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1580 - accuracy: 0.9414\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1520 - accuracy: 0.9435\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1553 - accuracy: 0.9435\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1525 - accuracy: 0.9418\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1515 - accuracy: 0.9446\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1456 - accuracy: 0.9436\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1461 - accuracy: 0.9454\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1499 - accuracy: 0.9441\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1437 - accuracy: 0.9460\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1417 - accuracy: 0.9494\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1403 - accuracy: 0.9473\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1386 - accuracy: 0.9487\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1350 - accuracy: 0.9499\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1389 - accuracy: 0.9487\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1343 - accuracy: 0.9506\n",
      "Epoch 84/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1309 - accuracy: 0.9522\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1303 - accuracy: 0.9516\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1303 - accuracy: 0.9519\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1287 - accuracy: 0.9527\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1285 - accuracy: 0.9545\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1233 - accuracy: 0.9566\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1234 - accuracy: 0.9551\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1237 - accuracy: 0.9560\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1200 - accuracy: 0.9558\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1196 - accuracy: 0.9566\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1199 - accuracy: 0.9543\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1148 - accuracy: 0.9591\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1180 - accuracy: 0.9569\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1104 - accuracy: 0.9610\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1143 - accuracy: 0.9586\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1105 - accuracy: 0.9609\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1078 - accuracy: 0.9616\n",
      "Model Trained!\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9256\n",
      "\n",
      "Accuracy = 92.56%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DjcboPmTc5FN",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "811ef565-bbba-4dec-ace1-6bfc3bec3dad"
   },
   "source": [
    "\"\"\"\n",
    "Fake news detection\n",
    "The Keras version of neural network\n",
    "\"\"\"\n",
    "\n",
    "from getEmbeddings import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "\n",
    "from keras import losses\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "\n",
    "# Read the data\n",
    "xtr = np.load('/content/drive/My Drive/Colab Notebooks/xtr.npy')\n",
    "xte = np.load('/content/drive/My Drive/Colab Notebooks/xte.npy')\n",
    "ytr = np.load('/content/drive/My Drive/Colab Notebooks/ytr.npy')\n",
    "yte = np.load('/content/drive/My Drive/Colab Notebooks/yte.npy')\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    '''Neural network with 3 hidden layers'''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=300, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(80, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(2, activation=\"softmax\", kernel_initializer='normal'))\n",
    "\n",
    "    # gradient descent\n",
    "    epochs=100\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = baseline_model()\n",
    "model.summary()\n",
    "x_train, x_test, y_train, y_test = train_test_split(xtr, ytr, test_size=0.2, random_state=42)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y = np_utils.to_categorical((label_encoder.transform(y_train)))\n",
    "label_encoder.fit(y_test)\n",
    "encoded_y_test = np_utils.to_categorical((label_encoder.transform(y_test)))\n",
    "\n",
    "\n",
    "epochs = 60\n",
    "learning_rate = 0.1 # initial learning rate\n",
    "decay_rate = 0.1\n",
    "momentum = 0.8\n",
    "# define the learning rate change \n",
    "def exp_decay(epoch):\n",
    "    lrate = learning_rate * np.exp(-decay_rate*epoch)\n",
    "    return lrate\n",
    "    \n",
    "# learning schedule callback\n",
    "loss_history = History()\n",
    "lr_rate = LearningRateScheduler(exp_decay)\n",
    "callbacks_list = [loss_history, lr_rate]\n",
    "\n",
    "# you invoke the LearningRateScheduler during the .fit() phase\n",
    "exponential_decay_model_history = model.fit(x_train, encoded_y,\n",
    "                                    batch_size=64,\n",
    "                                    epochs=100,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    verbose=1,\n",
    "                                    validation_data=(x_test, encoded_y_test))\n",
    "\n",
    "#estimator = model.fit(x_train, encoded_y, epochs=100, batch_size=64)\n",
    "print(\"Model Trained!\")\n",
    "score = model.evaluate(x_test, encoded_y_test)\n",
    "print(\"\")\n",
    "print(\"Accuracy = \" + format(score[1]*100, '.2f') + \"%\") \n",
    "probabs = model.predict_proba(x_test)\n",
    "y_pred = np.argmax(probabs, axis=1)\n",
    " "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_170 (Dense)            (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 80)                20560     \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 2)                 162       \n",
      "=================================================================\n",
      "Total params: 295,154\n",
      "Trainable params: 295,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.3750 - accuracy: 0.8327 - val_loss: 0.2678 - val_accuracy: 0.8940\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2656 - accuracy: 0.8973 - val_loss: 0.2480 - val_accuracy: 0.9106\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2351 - accuracy: 0.9104 - val_loss: 0.2299 - val_accuracy: 0.9139\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2190 - accuracy: 0.9158 - val_loss: 0.2174 - val_accuracy: 0.9145\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.2036 - accuracy: 0.9236 - val_loss: 0.2115 - val_accuracy: 0.9178\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1901 - accuracy: 0.9250 - val_loss: 0.2108 - val_accuracy: 0.9175\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1829 - accuracy: 0.9316 - val_loss: 0.2171 - val_accuracy: 0.9232\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1793 - accuracy: 0.9317 - val_loss: 0.2062 - val_accuracy: 0.9229\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1636 - accuracy: 0.9384 - val_loss: 0.2052 - val_accuracy: 0.9272\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1566 - accuracy: 0.9408 - val_loss: 0.2030 - val_accuracy: 0.9241\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1486 - accuracy: 0.9432 - val_loss: 0.2070 - val_accuracy: 0.9275\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1390 - accuracy: 0.9489 - val_loss: 0.2025 - val_accuracy: 0.9272\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1313 - accuracy: 0.9527 - val_loss: 0.2118 - val_accuracy: 0.9253\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1296 - accuracy: 0.9523 - val_loss: 0.2112 - val_accuracy: 0.9259\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1251 - accuracy: 0.9551 - val_loss: 0.2012 - val_accuracy: 0.9244\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1150 - accuracy: 0.9576 - val_loss: 0.2150 - val_accuracy: 0.9269\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1091 - accuracy: 0.9601 - val_loss: 0.2083 - val_accuracy: 0.9287\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1056 - accuracy: 0.9624 - val_loss: 0.2232 - val_accuracy: 0.9299\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.1010 - accuracy: 0.9627 - val_loss: 0.2099 - val_accuracy: 0.9308\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0969 - accuracy: 0.9664 - val_loss: 0.2239 - val_accuracy: 0.9281\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0882 - accuracy: 0.9700 - val_loss: 0.2337 - val_accuracy: 0.9272\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0890 - accuracy: 0.9678 - val_loss: 0.2214 - val_accuracy: 0.9269\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0821 - accuracy: 0.9715 - val_loss: 0.2401 - val_accuracy: 0.9266\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0806 - accuracy: 0.9713 - val_loss: 0.2339 - val_accuracy: 0.9302\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0773 - accuracy: 0.9730 - val_loss: 0.2442 - val_accuracy: 0.9281\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0789 - accuracy: 0.9725 - val_loss: 0.2472 - val_accuracy: 0.9287\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0762 - accuracy: 0.9733 - val_loss: 0.2275 - val_accuracy: 0.9308\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0681 - accuracy: 0.9780 - val_loss: 0.2423 - val_accuracy: 0.9287\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0634 - accuracy: 0.9780 - val_loss: 0.2495 - val_accuracy: 0.9266\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0659 - accuracy: 0.9770 - val_loss: 0.2691 - val_accuracy: 0.9238\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0644 - accuracy: 0.9785 - val_loss: 0.2573 - val_accuracy: 0.9302\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0602 - accuracy: 0.9785 - val_loss: 0.2538 - val_accuracy: 0.9278\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0601 - accuracy: 0.9799 - val_loss: 0.2619 - val_accuracy: 0.9287\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0576 - accuracy: 0.9810 - val_loss: 0.2549 - val_accuracy: 0.9284\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0563 - accuracy: 0.9828 - val_loss: 0.2595 - val_accuracy: 0.9281\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0573 - accuracy: 0.9797 - val_loss: 0.2715 - val_accuracy: 0.9259\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9838 - val_loss: 0.2680 - val_accuracy: 0.9290\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0507 - accuracy: 0.9824 - val_loss: 0.2766 - val_accuracy: 0.9290\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0514 - accuracy: 0.9834 - val_loss: 0.2747 - val_accuracy: 0.9290\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0527 - accuracy: 0.9831 - val_loss: 0.2784 - val_accuracy: 0.9275\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0487 - accuracy: 0.9840 - val_loss: 0.2780 - val_accuracy: 0.9269\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0507 - accuracy: 0.9838 - val_loss: 0.2706 - val_accuracy: 0.9272\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0487 - accuracy: 0.9843 - val_loss: 0.2912 - val_accuracy: 0.9269\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0458 - accuracy: 0.9855 - val_loss: 0.2867 - val_accuracy: 0.9275\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0464 - accuracy: 0.9850 - val_loss: 0.2812 - val_accuracy: 0.9290\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0485 - accuracy: 0.9833 - val_loss: 0.2809 - val_accuracy: 0.9272\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0497 - accuracy: 0.9834 - val_loss: 0.2861 - val_accuracy: 0.9259\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0467 - accuracy: 0.9858 - val_loss: 0.2809 - val_accuracy: 0.9269\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0461 - accuracy: 0.9852 - val_loss: 0.2843 - val_accuracy: 0.9247\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0466 - accuracy: 0.9857 - val_loss: 0.2912 - val_accuracy: 0.9232\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0436 - accuracy: 0.9865 - val_loss: 0.2888 - val_accuracy: 0.9250\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0479 - accuracy: 0.9851 - val_loss: 0.2835 - val_accuracy: 0.9262\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0438 - accuracy: 0.9859 - val_loss: 0.2862 - val_accuracy: 0.9262\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9868 - val_loss: 0.2831 - val_accuracy: 0.9266\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0436 - accuracy: 0.9851 - val_loss: 0.2838 - val_accuracy: 0.9259\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9853 - val_loss: 0.2850 - val_accuracy: 0.9262\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0443 - accuracy: 0.9861 - val_loss: 0.2838 - val_accuracy: 0.9278\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0433 - accuracy: 0.9870 - val_loss: 0.2891 - val_accuracy: 0.9266\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0442 - accuracy: 0.9859 - val_loss: 0.2859 - val_accuracy: 0.9266\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0428 - accuracy: 0.9873 - val_loss: 0.2848 - val_accuracy: 0.9256\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0446 - accuracy: 0.9862 - val_loss: 0.2847 - val_accuracy: 0.9259\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0378 - accuracy: 0.9884 - val_loss: 0.2898 - val_accuracy: 0.9262\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0408 - accuracy: 0.9866 - val_loss: 0.2907 - val_accuracy: 0.9266\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0420 - accuracy: 0.9871 - val_loss: 0.2912 - val_accuracy: 0.9266\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0397 - accuracy: 0.9887 - val_loss: 0.2905 - val_accuracy: 0.9272\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0442 - accuracy: 0.9871 - val_loss: 0.2930 - val_accuracy: 0.9266\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0439 - accuracy: 0.9864 - val_loss: 0.2916 - val_accuracy: 0.9269\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0399 - accuracy: 0.9872 - val_loss: 0.2917 - val_accuracy: 0.9275\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0440 - accuracy: 0.9851 - val_loss: 0.2919 - val_accuracy: 0.9272\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0420 - accuracy: 0.9865 - val_loss: 0.2919 - val_accuracy: 0.9275\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0418 - accuracy: 0.9865 - val_loss: 0.2902 - val_accuracy: 0.9262\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0416 - accuracy: 0.9871 - val_loss: 0.2903 - val_accuracy: 0.9269\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 0.2906 - val_accuracy: 0.9269\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0442 - accuracy: 0.9858 - val_loss: 0.2916 - val_accuracy: 0.9269\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0430 - accuracy: 0.9864 - val_loss: 0.2913 - val_accuracy: 0.9266\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0447 - accuracy: 0.9855 - val_loss: 0.2917 - val_accuracy: 0.9269\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0449 - accuracy: 0.9858 - val_loss: 0.2920 - val_accuracy: 0.9266\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0396 - accuracy: 0.9883 - val_loss: 0.2918 - val_accuracy: 0.9269\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0420 - accuracy: 0.9868 - val_loss: 0.2912 - val_accuracy: 0.9269\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0426 - accuracy: 0.9869 - val_loss: 0.2919 - val_accuracy: 0.9269\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9858 - val_loss: 0.2919 - val_accuracy: 0.9275\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0463 - accuracy: 0.9855 - val_loss: 0.2914 - val_accuracy: 0.9266\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 0.2915 - val_accuracy: 0.9266\n",
      "Epoch 84/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0406 - accuracy: 0.9874 - val_loss: 0.2914 - val_accuracy: 0.9266\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0376 - accuracy: 0.9887 - val_loss: 0.2917 - val_accuracy: 0.9266\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0430 - accuracy: 0.9873 - val_loss: 0.2916 - val_accuracy: 0.9269\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0379 - accuracy: 0.9880 - val_loss: 0.2916 - val_accuracy: 0.9269\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0422 - accuracy: 0.9864 - val_loss: 0.2916 - val_accuracy: 0.9269\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.2914 - val_accuracy: 0.9269\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0387 - accuracy: 0.9880 - val_loss: 0.2915 - val_accuracy: 0.9269\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0419 - accuracy: 0.9866 - val_loss: 0.2913 - val_accuracy: 0.9269\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 0.2914 - val_accuracy: 0.9269\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0444 - accuracy: 0.9865 - val_loss: 0.2913 - val_accuracy: 0.9266\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0412 - accuracy: 0.9874 - val_loss: 0.2913 - val_accuracy: 0.9269\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0406 - accuracy: 0.9874 - val_loss: 0.2915 - val_accuracy: 0.9266\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9868 - val_loss: 0.2914 - val_accuracy: 0.9266\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0388 - accuracy: 0.9882 - val_loss: 0.2915 - val_accuracy: 0.9269\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0395 - accuracy: 0.9871 - val_loss: 0.2915 - val_accuracy: 0.9266\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0424 - accuracy: 0.9867 - val_loss: 0.2915 - val_accuracy: 0.9266\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 0.0408 - accuracy: 0.9876 - val_loss: 0.2915 - val_accuracy: 0.9269\n",
      "Model Trained!\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.9269\n",
      "\n",
      "Accuracy = 92.69%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fecte_iShMOD",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "outputId": "483da948-b71f-4946-d20b-fd78f58d1f3f"
   },
   "source": [
    "\n",
    "\"\"\"\n",
    "Fake news detection\n",
    "LSTM model\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import os\n",
    "import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "top_words = 5000\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Read the text data\n",
    "if not os.path.isfile('./xtr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./xte_shuffled.npy') or \\\n",
    "    not os.path.isfile('./ytr_shuffled.npy') or \\\n",
    "    not os.path.isfile('./yte_shuffled.npy'):\n",
    "    getEmbeddings.clean_data()\n",
    "\n",
    "\n",
    "xtr = np.load('./xtr_shuffled.npy')\n",
    "xte = np.load('./xte_shuffled.npy')\n",
    "y_train = np.load('./ytr_shuffled.npy')\n",
    "y_test = np.load('./yte_shuffled.npy')\n",
    "\n",
    "cnt = Counter()\n",
    "x_train = []\n",
    "for x in xtr:\n",
    "    x_train.append(x.split())\n",
    "    for word in x_train[-1]:\n",
    "        cnt[word] += 1  \n",
    "\n",
    "# Storing most common words\n",
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_train:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Delete the short news\n",
    "i = 0\n",
    "while i < len(x_train):\n",
    "    if len(x_train[i]) > 10:\n",
    "        i += 1\n",
    "    else:\n",
    "        del x_train[i]\n",
    "        del y_train[i]\n",
    "\n",
    "# Generating test data\n",
    "x_test = []\n",
    "for x in xte:\n",
    "    x_test.append(x.split())\n",
    "\n",
    "# Encode the sentences\n",
    "for news in x_test:\n",
    "    i = 0\n",
    "    while i < len(news):\n",
    "        if news[i] in word_bank:\n",
    "            news[i] = word_bank[news[i]]\n",
    "            i += 1\n",
    "        else:\n",
    "            del news[i]\n",
    "\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_num, batch_size=batch_size)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy= %.2f%%\" % (scores[1]*100))\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-cbf7815f6375>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;31m# Read the text data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'./xtr_shuffled.npy'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m     \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'./xte_shuffled.npy'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m     \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'./ytr_shuffled.npy'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m     \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'./yte_shuffled.npy'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m     \u001B[0mgetEmbeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclean_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/drive/My Drive/Colab Notebooks/getEmbeddings.py\u001B[0m in \u001B[0;36mclean_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtext_train_arrays\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext_test_arrays\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_labels\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 84\u001B[0;31m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mclean_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mparser_f\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[1;32m    674\u001B[0m         )\n\u001B[1;32m    675\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 676\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    677\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    678\u001B[0m     \u001B[0mparser_f\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    446\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    447\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 448\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    449\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    878\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    879\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 880\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    881\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    882\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1112\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1113\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"c\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1114\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1115\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1116\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"python\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   1889\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"usecols\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1891\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1892\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1893\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] File datasets/train.csv does not exist: 'datasets/train.csv'"
     ]
    }
   ]
  }
 ]
}